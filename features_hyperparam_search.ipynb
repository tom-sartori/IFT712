{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Hyperparameter search\n",
    "This notebook is dedicated to hyperparameter search for the different classifiers that we chose to use for leaves classification base on features extracted from the images.\n",
    "\n",
    "The goal is to find the best hyperparameters for each classifier using cross validation to compare the performances between the classifiers with the default hyperparameters and the classifiers with the best hyperparameters."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Importing our own functions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "metadata": {},
   "outputs": [],
   "source": [
    "import importlib\n",
    "\n",
    "import src.Data as Data\n",
    "importlib.reload(Data)\n",
    "Data = Data.Data\n",
    "\n",
    "import src.Metrics as Metrics\n",
    "importlib.reload(Metrics)\n",
    "Metrics = Metrics.Metrics"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Importing libraries\n",
    "`numpy` and `pandas` are used to manipulate the data\n",
    "\n",
    "`scikit-learn` is used to train the classification models and compute the metrics\n",
    "\n",
    "`matplotlib` and `seaborn` are used to plot the results"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "metadata": {},
   "outputs": [],
   "source": [
    "import matplotlib.pyplot as plt\n",
    "from sklearn import metrics\n",
    "from sklearn.model_selection import LearningCurveDisplay, learning_curve, cross_validate, train_test_split, cross_val_predict\n",
    "from sklearn.metrics import accuracy_score, f1_score, precision_score, recall_score\n",
    "import numpy as np\n",
    "import pandas as pd"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Loading the data\n",
    "The data is loaded from the `data` folder.\n",
    "\n",
    "Samples are split into a training set and a test set with a custom ratio. Stratified sampling is used to ensure that the proportion of samples in each class is the same in both sets.\n",
    "\n",
    "The number of samples in the least represented class is computed to choose the number of folds for cross-validation."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Least populated class count: 8\n",
      "This is the maximum valid number of folds for cross validation.\n"
     ]
    }
   ],
   "source": [
    "data: Data = Data(test_size=0.2, include_images=False)\n",
    "\n",
    "least_populated_class_count = np.unique(data.y_train, return_counts=True)[1].min()\n",
    "print(\"Least populated class count:\", least_populated_class_count)\n",
    "print(\"This is the maximum valid number of folds for cross validation.\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Choosing the models\n",
    "Here you can choose which models you want include in the hyperparameter search.\n",
    "\n",
    "The parameter `n_jobs` is used to specify the number of cores to use for parallel processing. If `-1` is given, all cores are used."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.tree import DecisionTreeClassifier\n",
    "from sklearn.ensemble import RandomForestClassifier, BaggingClassifier\n",
    "from sklearn.linear_model import LogisticRegression\n",
    "from sklearn.svm import SVC\n",
    "from sklearn.naive_bayes import GaussianNB\n",
    "from sklearn.linear_model import SGDClassifier\n",
    "from sklearn.neighbors import KNeighborsClassifier\n",
    "from sklearn.ensemble import GradientBoostingClassifier\n",
    "from sklearn.neural_network import MLPClassifier\n",
    "from sklearn.ensemble import AdaBoostClassifier\n",
    "\n",
    "classifiers = [\n",
    "    # DecisionTreeClassifier(), \n",
    "    # RandomForestClassifier(n_jobs=-1), \n",
    "    # BaggingClassifier(n_jobs=-1), \n",
    "    # LogisticRegression(n_jobs=-1), \n",
    "    # SVC(), \n",
    "    # GaussianNB(), \n",
    "    # SGDClassifier(n_jobs=-1), \n",
    "    # KNeighborsClassifier(n_jobs=-1), \n",
    "    GradientBoostingClassifier(), \n",
    "    # MLPClassifier(), \n",
    "    AdaBoostClassifier()\n",
    "]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Getting the list of hyperparameters\n",
    "To simplify the hyperparameter search, we use the `get_params()` method of the classifier to get the list of hyperparameters that can be tuned.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Classifier: GradientBoostingClassifier\n",
      "Parameters:\n",
      "\t ccp_alpha\n",
      "\t criterion\n",
      "\t init\n",
      "\t learning_rate\n",
      "\t loss\n",
      "\t max_depth\n",
      "\t max_features\n",
      "\t max_leaf_nodes\n",
      "\t min_impurity_decrease\n",
      "\t min_samples_leaf\n",
      "\t min_samples_split\n",
      "\t min_weight_fraction_leaf\n",
      "\t n_estimators\n",
      "\t n_iter_no_change\n",
      "\t random_state\n",
      "\t subsample\n",
      "\t tol\n",
      "\t validation_fraction\n",
      "\t verbose\n",
      "\t warm_start\n",
      "\n",
      "Classifier: AdaBoostClassifier\n",
      "Parameters:\n",
      "\t algorithm\n",
      "\t base_estimator\n",
      "\t estimator\n",
      "\t learning_rate\n",
      "\t n_estimators\n",
      "\t random_state\n",
      "\n"
     ]
    }
   ],
   "source": [
    "for classifier in classifiers:\n",
    "    print(\"Classifier:\", classifier.__class__.__name__)\n",
    "    print(\"Parameters:\")\n",
    "    for key in classifier.get_params():\n",
    "        print(\"\\t\", key)\n",
    "    print(\"\")\n",
    "    "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Chosing the hyperparameters to tune\n",
    "We then need to choose from the list above which hyperparameters we want to tune. We can also choose the range of values to test for each hyperparameter.\n",
    "\n",
    "The `param_grid` variable is a dictionary where the keys are the names of the hyperparameters and the values are the list of values to test for each hyperparameter."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 43,
   "metadata": {},
   "outputs": [],
   "source": [
    "param_grids = []\n",
    "\n",
    "# DecisionTreeClassifier\n",
    "param_grid = {\n",
    "    \"criterion\": [\"gini\", \"entropy\"],\n",
    "    \"splitter\": [\"best\", \"random\"],\n",
    "    \"max_depth\": [None, 5, 10, 20, 50, 100],\n",
    "    \"min_samples_split\": [2, 5, 10],\n",
    "    \"min_samples_leaf\": [1, 2, 5, 10],\n",
    "    \"max_features\": [\"sqrt\", \"log2\"]\n",
    "}\n",
    "if \"DecisionTreeClassifier\" in [classifier.__class__.__name__ for classifier in classifiers]:\n",
    "    param_grids.append(param_grid)\n",
    "\n",
    "# RandomForestClassifier\n",
    "param_grid = {\n",
    "    \"n_estimators\": [10, 50, 100, 200, 500],\n",
    "    \"criterion\": [\"gini\", \"entropy\"],\n",
    "    \"min_samples_split\": [2, 5, 10],\n",
    "    \"max_features\": [\"sqrt\", \"log2\"]\n",
    "}\n",
    "if \"RandomForestClassifier\" in [classifier.__class__.__name__ for classifier in classifiers]:\n",
    "    param_grids.append(param_grid)\n",
    "\n",
    "# BaggingClassifier\n",
    "param_grid = {\n",
    "    \"n_estimators\": [10, 20, 50, 100],\n",
    "    \"max_samples\": [0.1, 0.5, 1.0],\n",
    "    \"max_features\": [0.1, 0.5, 1.0],\n",
    "    \"bootstrap\": [True, False],\n",
    "    \"bootstrap_features\": [True, False]\n",
    "}\n",
    "if \"BaggingClassifier\" in [classifier.__class__.__name__ for classifier in classifiers]:\n",
    "    param_grids.append(param_grid)\n",
    "\n",
    "# LogisticRegression\n",
    "param_grid = {\n",
    "    \"penalty\": [\"l1\", \"l2\", \"elasticnet\"],\n",
    "    \"C\": [0.1, 0.5, 2, 5, 10, 20, 50, 100, 200, 500, 1000],\n",
    "    \"solver\": [\"newton-cg\", \"lbfgs\", \"liblinear\", \"sag\", \"saga\"],\n",
    "    \"max_iter\": [100, 200, 500]\n",
    "}\n",
    "if \"LogisticRegression\" in [classifier.__class__.__name__ for classifier in classifiers]:\n",
    "    param_grids.append(param_grid)\n",
    "\n",
    "# SVC\n",
    "param_grid = {\n",
    "    \"kernel\": [\"linear\", \"poly\", \"rbf\", \"sigmoid\"],\n",
    "    \"C\": [0.1, 0.5, 2, 5, 10, 20, 50, 100, 200, 500, 1000],\n",
    "    \"gamma\": [\"scale\", \"auto\"]\n",
    "}\n",
    "if \"SVC\" in [classifier.__class__.__name__ for classifier in classifiers]:\n",
    "    param_grids.append(param_grid)\n",
    "\n",
    "# GaussianNB\n",
    "param_grid = {\n",
    "    \"var_smoothing\": [1e-07, 1e-06, 1e-05, 1e-04, 1e-03, 0.005, 0.01, 0.02, 0.05, 0.075, 0.1]\n",
    "}\n",
    "if \"GaussianNB\" in [classifier.__class__.__name__ for classifier in classifiers]:\n",
    "    param_grids.append(param_grid)\n",
    "\n",
    "# SGDClassifier\n",
    "param_grid = {\n",
    "    \"loss\": [\"hinge\", \"log\", \"modified_huber\", \"squared_hinge\", \"perceptron\"],\n",
    "    \"penalty\": [\"l1\", \"l2\", \"elasticnet\"],\n",
    "    \"alpha\": [0.00001, 0.0001, 0.001, 0.01],\n",
    "    \"max_iter\": [1000, 2000, 5000, 10000],\n",
    "}\n",
    "if \"SGDClassifier\" in [classifier.__class__.__name__ for classifier in classifiers]:\n",
    "    param_grids.append(param_grid)\n",
    "\n",
    "# KNeighborsClassifier\n",
    "param_grid = {\n",
    "    \"n_neighbors\": [1, 2, 5, 10],\n",
    "    \"weights\": [\"uniform\", \"distance\"],\n",
    "    \"algorithm\": [\"auto\", \"ball_tree\", \"kd_tree\", \"brute\"],\n",
    "    \"leaf_size\": [1, 2, 5, 10, 20, 30, 50],\n",
    "    \"p\": [1, 2]\n",
    "}\n",
    "if \"KNeighborsClassifier\" in [classifier.__class__.__name__ for classifier in classifiers]:\n",
    "    param_grids.append(param_grid)\n",
    "\n",
    "# GradientBoostingClassifier\n",
    "param_grid = {\n",
    "    \"learning_rate\": [0.005, 0.01, 0.025, 0.05, 0.1, 0.5],\n",
    "    \"n_estimators\": [100, 500], \n",
    "    \"criterion\": [\"friedman_mse\", \"squared_error\"],\n",
    "    \"max_depth\": [1, 2, 3, 5, 10],\n",
    "    \"min_samples_split\": [2, 5, 10, 15, 20],\n",
    "    \"max_features\": [\"sqrt\", \"log2\"]\n",
    "}\n",
    "if \"GradientBoostingClassifier\" in [classifier.__class__.__name__ for classifier in classifiers]:\n",
    "    param_grids.append(param_grid)\n",
    "\n",
    "# MLPClassifier\n",
    "param_grid = {\n",
    "    \"hidden_layer_sizes\":  [(20,20,), (100,), (200,), (500,)],\n",
    "    \"activation\": [\"logistic\", \"tanh\", \"relu\"],\n",
    "    \"solver\": [\"lbfgs\", \"sgd\"],\n",
    "    \"alpha\": [0.00001, 0.0001, 0.001, 0.01] ,\n",
    "    \"learning_rate\": [\"constant\", \"adaptive\"],\n",
    "    \"max_iter\": [200, 500]\n",
    "}\n",
    "if \"MLPClassifier\" in [classifier.__class__.__name__ for classifier in classifiers]:\n",
    "    param_grids.append(param_grid)\n",
    "\n",
    "# AdaBoostClassifier\n",
    "param_grid = {\n",
    "    \"n_estimators\": [50, 200, 500],\n",
    "    \"learning_rate\": [0.001, 0.01, 0.1, 0.5],\n",
    "    \"algorithm\": [\"SAMME\", \"SAMME.R\"]\n",
    "}\n",
    "if \"AdaBoostClassifier\" in [classifier.__class__.__name__ for classifier in classifiers]:\n",
    "    param_grids.append(param_grid)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Fitting the models with all combinations of hyperparameters\n",
    "We use the `GridSearchCV` class to fit the models with all combinations of hyperparameters and find the best hyperparameters for each model. \n",
    "\n",
    "This class uses cross-validation to evaluate the performance through an exhaustive search over the hyperparameter values space."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 44,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Classifier: GradientBoostingClassifier\n",
      "Parameters:\n",
      "\tlearning_rate: [0.005, 0.01, 0.025, 0.05]\n",
      "\tn_estimators: [500]\n",
      "\tcriterion   : ['friedman_mse']\n",
      "\tmax_depth   : [1, 2, 3]\n",
      "\tmin_samples_split: [10, 15, 20]\n",
      "\tmax_features: ['log2']\n",
      "\n",
      "Fitting 2 folds for each of 36 candidates, totalling 72 fits\n"
     ]
    }
   ],
   "source": [
    "from sklearn.model_selection import GridSearchCV\n",
    "\n",
    "best_params = []\n",
    "best_scores = []\n",
    "\n",
    "for classifier, param_grid in zip(classifiers, param_grids):\n",
    "    print(\"Classifier:\", classifier.__class__.__name__)\n",
    "    print(\"Parameters:\")\n",
    "    for key in param_grid:\n",
    "        print(f\"\\t{key:12}: {param_grid[key]}\")\n",
    "    print(\"\")\n",
    "    \n",
    "    grid_search = GridSearchCV(classifier, param_grid, cv=2, verbose=1, n_jobs=-1)\n",
    "    grid_search.fit(data.x_train, data.y_train)\n",
    "    best_params.append(grid_search.best_params_)\n",
    "    best_scores.append(grid_search.best_score_)\n",
    "    print(\"Best parameters:\", best_params[-1])\n",
    "    print(f\"Best score: {best_scores[-1]:.3f}\")\n",
    "    print(\"\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Printing the results"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Classifier: GradientBoostingClassifier\n",
      "Best parameters:\n",
      "\tcriterion   : friedman_mse\n",
      "\tlearning_rate: 0.05\n",
      "\tmax_depth   : 3\n",
      "\tmax_features: log2\n",
      "\tmin_samples_split: 10\n",
      "\tn_estimators: 100\n",
      "Best score: 0.782\n",
      "\n",
      "Classifier: AdaBoostClassifier\n",
      "Best parameters:\n",
      "\talgorithm   : SAMME.R\n",
      "\tlearning_rate: 0.01\n",
      "\tn_estimators: 500\n",
      "Best score: 0.535\n",
      "\n"
     ]
    }
   ],
   "source": [
    "for classifier, best_param, best_scores in zip(classifiers, best_params, best_scores):\n",
    "    print(\"Classifier:\", classifier.__class__.__name__)\n",
    "    print(\"Best parameters:\")\n",
    "    for key in best_param:\n",
    "        print(f\"\\t{key:12}: {best_param[key]}\")\n",
    "    print(f\"Best score: {best_scores:.3f}\")\n",
    "    print(\"\")"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
