{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Hyperparameter search\n",
    "This notebook is dedicated to hyperparameter search for the different classifiers that we chose to use for leaves classification base on the images.\n",
    "\n",
    "The goal is to find the best hyperparameters for each classifier using cross validation to compare the performances between the classifiers with the default hyperparameters and the classifiers with the best hyperparameters."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Importing our own functions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import importlib\n",
    "\n",
    "import src.Data as Data\n",
    "importlib.reload(Data)\n",
    "Data = Data.Data\n",
    "\n",
    "import src.Metrics as Metrics\n",
    "importlib.reload(Metrics)\n",
    "Metrics = Metrics.Metrics"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Importing libraries\n",
    "`numpy` and `pandas` are used to manipulate the data\n",
    "\n",
    "`scikit-learn` is used to train the classification models and compute the metrics\n",
    "\n",
    "`matplotlib` and `seaborn` are used to plot the results"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "import matplotlib.pyplot as plt\n",
    "from sklearn import metrics\n",
    "from sklearn.model_selection import LearningCurveDisplay, learning_curve, cross_validate, train_test_split, cross_val_predict\n",
    "from sklearn.metrics import accuracy_score, f1_score, precision_score, recall_score\n",
    "import numpy as np\n",
    "import pandas as pd"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Loading the data\n",
    "The data is loaded from the `data` folder.\n",
    "\n",
    "Samples are split into a training set and a test set with a custom ratio. Stratified sampling is used to ensure that the proportion of samples in each class is the same in both sets.\n",
    "\n",
    "The number of samples in the least represented class is computed to choose the number of folds for cross-validation."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Least populated class count: 8\n",
      "This is the maximum valid number of folds for cross validation.\n"
     ]
    }
   ],
   "source": [
    "data: Data = Data(test_size=0.2, include_images=True)\n",
    "\n",
    "least_populated_class_count = np.unique(data.y_train, return_counts=True)[1].min()\n",
    "print(\"Least populated class count:\", least_populated_class_count)\n",
    "print(\"This is the maximum valid number of folds for cross validation.\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Choosing the models\n",
    "Here you can choose which models you want include in the hyperparameter search.\n",
    "\n",
    "The parameter `n_jobs` is used to specify the number of cores to use for parallel processing. If `-1` is given, all cores are used."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.ensemble import RandomForestClassifier\n",
    "from sklearn.svm import SVC\n",
    "from sklearn.neighbors import KNeighborsClassifier\n",
    "from sklearn.ensemble import GradientBoostingClassifier\n",
    "from sklearn.ensemble import AdaBoostClassifier\n",
    "\n",
    "classifiers = [\n",
    "    RandomForestClassifier(), \n",
    "    SVC(), \n",
    "    KNeighborsClassifier(), \n",
    "    GradientBoostingClassifier(), \n",
    "    AdaBoostClassifier()\n",
    "]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Getting the list of hyperparameters\n",
    "To simplify the hyperparameter search, we use the `get_params()` method of the classifier to get the list of hyperparameters that can be tuned.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Classifier: RandomForestClassifier\n",
      "Parameters:\n",
      "\t bootstrap\n",
      "\t ccp_alpha\n",
      "\t class_weight\n",
      "\t criterion\n",
      "\t max_depth\n",
      "\t max_features\n",
      "\t max_leaf_nodes\n",
      "\t max_samples\n",
      "\t min_impurity_decrease\n",
      "\t min_samples_leaf\n",
      "\t min_samples_split\n",
      "\t min_weight_fraction_leaf\n",
      "\t n_estimators\n",
      "\t n_jobs\n",
      "\t oob_score\n",
      "\t random_state\n",
      "\t verbose\n",
      "\t warm_start\n",
      "\n",
      "Classifier: SVC\n",
      "Parameters:\n",
      "\t C\n",
      "\t break_ties\n",
      "\t cache_size\n",
      "\t class_weight\n",
      "\t coef0\n",
      "\t decision_function_shape\n",
      "\t degree\n",
      "\t gamma\n",
      "\t kernel\n",
      "\t max_iter\n",
      "\t probability\n",
      "\t random_state\n",
      "\t shrinking\n",
      "\t tol\n",
      "\t verbose\n",
      "\n",
      "Classifier: KNeighborsClassifier\n",
      "Parameters:\n",
      "\t algorithm\n",
      "\t leaf_size\n",
      "\t metric\n",
      "\t metric_params\n",
      "\t n_jobs\n",
      "\t n_neighbors\n",
      "\t p\n",
      "\t weights\n",
      "\n",
      "Classifier: GradientBoostingClassifier\n",
      "Parameters:\n",
      "\t ccp_alpha\n",
      "\t criterion\n",
      "\t init\n",
      "\t learning_rate\n",
      "\t loss\n",
      "\t max_depth\n",
      "\t max_features\n",
      "\t max_leaf_nodes\n",
      "\t min_impurity_decrease\n",
      "\t min_samples_leaf\n",
      "\t min_samples_split\n",
      "\t min_weight_fraction_leaf\n",
      "\t n_estimators\n",
      "\t n_iter_no_change\n",
      "\t random_state\n",
      "\t subsample\n",
      "\t tol\n",
      "\t validation_fraction\n",
      "\t verbose\n",
      "\t warm_start\n",
      "\n",
      "Classifier: AdaBoostClassifier\n",
      "Parameters:\n",
      "\t algorithm\n",
      "\t base_estimator\n",
      "\t estimator\n",
      "\t learning_rate\n",
      "\t n_estimators\n",
      "\t random_state\n",
      "\n"
     ]
    }
   ],
   "source": [
    "for classifier in classifiers:\n",
    "    print(\"Classifier:\", classifier.__class__.__name__)\n",
    "    print(\"Parameters:\")\n",
    "    for key in classifier.get_params():\n",
    "        print(\"\\t\", key)\n",
    "    print(\"\")\n",
    "    "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Chosing the hyperparameters to tune\n",
    "We then need to choose from the list above which hyperparameters we want to tune. We can also choose the range of values to test for each hyperparameter.\n",
    "\n",
    "The `param_grid` variable is a dictionary where the keys are the names of the hyperparameters and the values are the list of values to test for each hyperparameter."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "param_grids = []\n",
    "\n",
    "# RandomForestClassifier\n",
    "param_grid = {\n",
    "    \"n_estimators\": [10, 50, 100, 200, 500],\n",
    "    \"criterion\": [\"gini\", \"entropy\"],\n",
    "    \"min_samples_split\": [2, 5, 10],\n",
    "    \"max_features\": [\"sqrt\", \"log2\"]\n",
    "}\n",
    "if \"RandomForestClassifier\" in [classifier.__class__.__name__ for classifier in classifiers]:\n",
    "    param_grids.append(param_grid)\n",
    "\n",
    "# SVC\n",
    "param_grid = {\n",
    "    \"kernel\": [\"linear\", \"poly\", \"rbf\", \"sigmoid\"],\n",
    "    \"C\": [0.1, 0.5, 2, 5, 10, 20, 50, 100, 200, 500, 1000],\n",
    "    \"gamma\": [\"scale\", \"auto\"]\n",
    "}\n",
    "if \"SVC\" in [classifier.__class__.__name__ for classifier in classifiers]:\n",
    "    param_grids.append(param_grid)\n",
    "\n",
    "# KNeighborsClassifier\n",
    "param_grid = {\n",
    "    \"n_neighbors\": [1, 2, 5, 10],\n",
    "    \"weights\": [\"uniform\", \"distance\"],\n",
    "    \"algorithm\": [\"auto\", \"ball_tree\", \"kd_tree\", \"brute\"],\n",
    "    \"leaf_size\": [1, 2, 5, 10, 20, 30, 50],\n",
    "    \"p\": [1, 2]\n",
    "}\n",
    "if \"KNeighborsClassifier\" in [classifier.__class__.__name__ for classifier in classifiers]:\n",
    "    param_grids.append(param_grid)\n",
    "\n",
    "# GradientBoostingClassifier\n",
    "param_grid = {\n",
    "    \"learning_rate\": [0.005, 0.01, 0.025, 0.05, 0.1, 0.5],\n",
    "    \"n_estimators\": [100, 500], \n",
    "    \"criterion\": [\"friedman_mse\", \"squared_error\"],\n",
    "    \"max_depth\": [1, 2, 3, 5, 10],\n",
    "    \"min_samples_split\": [2, 5, 10, 15, 20],\n",
    "    \"max_features\": [\"sqrt\", \"log2\"]\n",
    "}\n",
    "if \"GradientBoostingClassifier\" in [classifier.__class__.__name__ for classifier in classifiers]:\n",
    "    param_grids.append(param_grid)\n",
    "\n",
    "# AdaBoostClassifier\n",
    "param_grid = {\n",
    "    \"n_estimators\": [50, 200, 500],\n",
    "    \"learning_rate\": [0.001, 0.01, 0.1, 0.5],\n",
    "    \"algorithm\": [\"SAMME\", \"SAMME.R\"]\n",
    "}\n",
    "if \"AdaBoostClassifier\" in [classifier.__class__.__name__ for classifier in classifiers]:\n",
    "    param_grids.append(param_grid)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Fitting the models with all combinations of hyperparameters\n",
    "We use the `GridSearchCV` class to fit the models with all combinations of hyperparameters and find the best hyperparameters for each model. \n",
    "\n",
    "This class uses cross-validation to evaluate the performance through an exhaustive search over the hyperparameter values space."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Classifier: RandomForestClassifier\n",
      "Parameters:\n",
      "\tn_estimators: [10, 50, 100, 200, 500]\n",
      "\tcriterion   : ['gini', 'entropy']\n",
      "\tmin_samples_split: [2, 5, 10]\n",
      "\tmax_features: ['sqrt', 'log2']\n",
      "\n",
      "Fitting 2 folds for each of 60 candidates, totalling 120 fits\n"
     ]
    },
    {
     "ename": "MemoryError",
     "evalue": "Unable to allocate 5.25 GiB for an array with shape (396, 1778337) and data type float64",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31m_RemoteTraceback\u001b[0m                          Traceback (most recent call last)",
      "\u001b[1;31m_RemoteTraceback\u001b[0m: \n\"\"\"\nTraceback (most recent call last):\n  File \"c:\\Python310\\lib\\site-packages\\joblib\\externals\\loky\\process_executor.py\", line 428, in _process_worker\n    r = call_item()\n  File \"c:\\Python310\\lib\\site-packages\\joblib\\externals\\loky\\process_executor.py\", line 275, in __call__\n    return self.fn(*self.args, **self.kwargs)\n  File \"c:\\Python310\\lib\\site-packages\\joblib\\_parallel_backends.py\", line 620, in __call__\n    return self.func(*args, **kwargs)\n  File \"c:\\Python310\\lib\\site-packages\\joblib\\parallel.py\", line 288, in __call__\n    return [func(*args, **kwargs)\n  File \"c:\\Python310\\lib\\site-packages\\joblib\\parallel.py\", line 288, in <listcomp>\n    return [func(*args, **kwargs)\n  File \"c:\\Python310\\lib\\site-packages\\sklearn\\utils\\parallel.py\", line 127, in __call__\n    return self.function(*args, **kwargs)\n  File \"c:\\Python310\\lib\\site-packages\\sklearn\\model_selection\\_validation.py\", line 722, in _fit_and_score\n    X_test, y_test = _safe_split(estimator, X, y, test, train)\n  File \"c:\\Python310\\lib\\site-packages\\sklearn\\utils\\metaestimators.py\", line 155, in _safe_split\n    X_subset = _safe_indexing(X, indices)\n  File \"c:\\Python310\\lib\\site-packages\\sklearn\\utils\\__init__.py\", line 355, in _safe_indexing\n    return _array_indexing(X, indices, indices_dtype, axis=axis)\n  File \"c:\\Python310\\lib\\site-packages\\sklearn\\utils\\__init__.py\", line 184, in _array_indexing\n    return array[key] if axis == 0 else array[:, key]\n  File \"C:\\Users\\LV\\AppData\\Roaming\\Python\\Python310\\site-packages\\numpy\\core\\memmap.py\", line 334, in __getitem__\n    res = super().__getitem__(index)\nnumpy.core._exceptions._ArrayMemoryError: Unable to allocate 5.25 GiB for an array with shape (396, 1778337) and data type float64\n\"\"\"",
      "\nThe above exception was the direct cause of the following exception:\n",
      "\u001b[1;31mMemoryError\u001b[0m                               Traceback (most recent call last)",
      "\u001b[1;32mc:\\Users\\LV\\OneDrive\\Documents\\_Cours\\UdS\\ML\\TP\\IFT712\\image_hyperparam_search.ipynb Cell 15\u001b[0m line \u001b[0;36m1\n\u001b[0;32m     <a href='vscode-notebook-cell:/c%3A/Users/LV/OneDrive/Documents/_Cours/UdS/ML/TP/IFT712/image_hyperparam_search.ipynb#X20sZmlsZQ%3D%3D?line=10'>11</a>\u001b[0m \u001b[39mprint\u001b[39m(\u001b[39m\"\u001b[39m\u001b[39m\"\u001b[39m)\n\u001b[0;32m     <a href='vscode-notebook-cell:/c%3A/Users/LV/OneDrive/Documents/_Cours/UdS/ML/TP/IFT712/image_hyperparam_search.ipynb#X20sZmlsZQ%3D%3D?line=12'>13</a>\u001b[0m grid_search \u001b[39m=\u001b[39m GridSearchCV(classifier, param_grid, cv\u001b[39m=\u001b[39m\u001b[39m2\u001b[39m, verbose\u001b[39m=\u001b[39m\u001b[39m1\u001b[39m, n_jobs\u001b[39m=\u001b[39m\u001b[39m-\u001b[39m\u001b[39m1\u001b[39m)\n\u001b[1;32m---> <a href='vscode-notebook-cell:/c%3A/Users/LV/OneDrive/Documents/_Cours/UdS/ML/TP/IFT712/image_hyperparam_search.ipynb#X20sZmlsZQ%3D%3D?line=13'>14</a>\u001b[0m grid_search\u001b[39m.\u001b[39;49mfit(data\u001b[39m.\u001b[39;49mx_image_train, data\u001b[39m.\u001b[39;49my_train)\n\u001b[0;32m     <a href='vscode-notebook-cell:/c%3A/Users/LV/OneDrive/Documents/_Cours/UdS/ML/TP/IFT712/image_hyperparam_search.ipynb#X20sZmlsZQ%3D%3D?line=14'>15</a>\u001b[0m best_params\u001b[39m.\u001b[39mappend(grid_search\u001b[39m.\u001b[39mbest_params_)\n\u001b[0;32m     <a href='vscode-notebook-cell:/c%3A/Users/LV/OneDrive/Documents/_Cours/UdS/ML/TP/IFT712/image_hyperparam_search.ipynb#X20sZmlsZQ%3D%3D?line=15'>16</a>\u001b[0m best_scores\u001b[39m.\u001b[39mappend(grid_search\u001b[39m.\u001b[39mbest_score_)\n",
      "File \u001b[1;32mc:\\Python310\\lib\\site-packages\\sklearn\\base.py:1152\u001b[0m, in \u001b[0;36m_fit_context.<locals>.decorator.<locals>.wrapper\u001b[1;34m(estimator, *args, **kwargs)\u001b[0m\n\u001b[0;32m   1145\u001b[0m     estimator\u001b[39m.\u001b[39m_validate_params()\n\u001b[0;32m   1147\u001b[0m \u001b[39mwith\u001b[39;00m config_context(\n\u001b[0;32m   1148\u001b[0m     skip_parameter_validation\u001b[39m=\u001b[39m(\n\u001b[0;32m   1149\u001b[0m         prefer_skip_nested_validation \u001b[39mor\u001b[39;00m global_skip_validation\n\u001b[0;32m   1150\u001b[0m     )\n\u001b[0;32m   1151\u001b[0m ):\n\u001b[1;32m-> 1152\u001b[0m     \u001b[39mreturn\u001b[39;00m fit_method(estimator, \u001b[39m*\u001b[39margs, \u001b[39m*\u001b[39m\u001b[39m*\u001b[39mkwargs)\n",
      "File \u001b[1;32mc:\\Python310\\lib\\site-packages\\sklearn\\model_selection\\_search.py:898\u001b[0m, in \u001b[0;36mBaseSearchCV.fit\u001b[1;34m(self, X, y, groups, **fit_params)\u001b[0m\n\u001b[0;32m    892\u001b[0m     results \u001b[39m=\u001b[39m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_format_results(\n\u001b[0;32m    893\u001b[0m         all_candidate_params, n_splits, all_out, all_more_results\n\u001b[0;32m    894\u001b[0m     )\n\u001b[0;32m    896\u001b[0m     \u001b[39mreturn\u001b[39;00m results\n\u001b[1;32m--> 898\u001b[0m \u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49m_run_search(evaluate_candidates)\n\u001b[0;32m    900\u001b[0m \u001b[39m# multimetric is determined here because in the case of a callable\u001b[39;00m\n\u001b[0;32m    901\u001b[0m \u001b[39m# self.scoring the return type is only known after calling\u001b[39;00m\n\u001b[0;32m    902\u001b[0m first_test_score \u001b[39m=\u001b[39m all_out[\u001b[39m0\u001b[39m][\u001b[39m\"\u001b[39m\u001b[39mtest_scores\u001b[39m\u001b[39m\"\u001b[39m]\n",
      "File \u001b[1;32mc:\\Python310\\lib\\site-packages\\sklearn\\model_selection\\_search.py:1422\u001b[0m, in \u001b[0;36mGridSearchCV._run_search\u001b[1;34m(self, evaluate_candidates)\u001b[0m\n\u001b[0;32m   1420\u001b[0m \u001b[39mdef\u001b[39;00m \u001b[39m_run_search\u001b[39m(\u001b[39mself\u001b[39m, evaluate_candidates):\n\u001b[0;32m   1421\u001b[0m \u001b[39m    \u001b[39m\u001b[39m\"\"\"Search all candidates in param_grid\"\"\"\u001b[39;00m\n\u001b[1;32m-> 1422\u001b[0m     evaluate_candidates(ParameterGrid(\u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49mparam_grid))\n",
      "File \u001b[1;32mc:\\Python310\\lib\\site-packages\\sklearn\\model_selection\\_search.py:845\u001b[0m, in \u001b[0;36mBaseSearchCV.fit.<locals>.evaluate_candidates\u001b[1;34m(candidate_params, cv, more_results)\u001b[0m\n\u001b[0;32m    837\u001b[0m \u001b[39mif\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mverbose \u001b[39m>\u001b[39m \u001b[39m0\u001b[39m:\n\u001b[0;32m    838\u001b[0m     \u001b[39mprint\u001b[39m(\n\u001b[0;32m    839\u001b[0m         \u001b[39m\"\u001b[39m\u001b[39mFitting \u001b[39m\u001b[39m{0}\u001b[39;00m\u001b[39m folds for each of \u001b[39m\u001b[39m{1}\u001b[39;00m\u001b[39m candidates,\u001b[39m\u001b[39m\"\u001b[39m\n\u001b[0;32m    840\u001b[0m         \u001b[39m\"\u001b[39m\u001b[39m totalling \u001b[39m\u001b[39m{2}\u001b[39;00m\u001b[39m fits\u001b[39m\u001b[39m\"\u001b[39m\u001b[39m.\u001b[39mformat(\n\u001b[0;32m    841\u001b[0m             n_splits, n_candidates, n_candidates \u001b[39m*\u001b[39m n_splits\n\u001b[0;32m    842\u001b[0m         )\n\u001b[0;32m    843\u001b[0m     )\n\u001b[1;32m--> 845\u001b[0m out \u001b[39m=\u001b[39m parallel(\n\u001b[0;32m    846\u001b[0m     delayed(_fit_and_score)(\n\u001b[0;32m    847\u001b[0m         clone(base_estimator),\n\u001b[0;32m    848\u001b[0m         X,\n\u001b[0;32m    849\u001b[0m         y,\n\u001b[0;32m    850\u001b[0m         train\u001b[39m=\u001b[39;49mtrain,\n\u001b[0;32m    851\u001b[0m         test\u001b[39m=\u001b[39;49mtest,\n\u001b[0;32m    852\u001b[0m         parameters\u001b[39m=\u001b[39;49mparameters,\n\u001b[0;32m    853\u001b[0m         split_progress\u001b[39m=\u001b[39;49m(split_idx, n_splits),\n\u001b[0;32m    854\u001b[0m         candidate_progress\u001b[39m=\u001b[39;49m(cand_idx, n_candidates),\n\u001b[0;32m    855\u001b[0m         \u001b[39m*\u001b[39;49m\u001b[39m*\u001b[39;49mfit_and_score_kwargs,\n\u001b[0;32m    856\u001b[0m     )\n\u001b[0;32m    857\u001b[0m     \u001b[39mfor\u001b[39;49;00m (cand_idx, parameters), (split_idx, (train, test)) \u001b[39min\u001b[39;49;00m product(\n\u001b[0;32m    858\u001b[0m         \u001b[39menumerate\u001b[39;49m(candidate_params), \u001b[39menumerate\u001b[39;49m(cv\u001b[39m.\u001b[39;49msplit(X, y, groups))\n\u001b[0;32m    859\u001b[0m     )\n\u001b[0;32m    860\u001b[0m )\n\u001b[0;32m    862\u001b[0m \u001b[39mif\u001b[39;00m \u001b[39mlen\u001b[39m(out) \u001b[39m<\u001b[39m \u001b[39m1\u001b[39m:\n\u001b[0;32m    863\u001b[0m     \u001b[39mraise\u001b[39;00m \u001b[39mValueError\u001b[39;00m(\n\u001b[0;32m    864\u001b[0m         \u001b[39m\"\u001b[39m\u001b[39mNo fits were performed. \u001b[39m\u001b[39m\"\u001b[39m\n\u001b[0;32m    865\u001b[0m         \u001b[39m\"\u001b[39m\u001b[39mWas the CV iterator empty? \u001b[39m\u001b[39m\"\u001b[39m\n\u001b[0;32m    866\u001b[0m         \u001b[39m\"\u001b[39m\u001b[39mWere there no candidates?\u001b[39m\u001b[39m\"\u001b[39m\n\u001b[0;32m    867\u001b[0m     )\n",
      "File \u001b[1;32mc:\\Python310\\lib\\site-packages\\sklearn\\utils\\parallel.py:65\u001b[0m, in \u001b[0;36mParallel.__call__\u001b[1;34m(self, iterable)\u001b[0m\n\u001b[0;32m     60\u001b[0m config \u001b[39m=\u001b[39m get_config()\n\u001b[0;32m     61\u001b[0m iterable_with_config \u001b[39m=\u001b[39m (\n\u001b[0;32m     62\u001b[0m     (_with_config(delayed_func, config), args, kwargs)\n\u001b[0;32m     63\u001b[0m     \u001b[39mfor\u001b[39;00m delayed_func, args, kwargs \u001b[39min\u001b[39;00m iterable\n\u001b[0;32m     64\u001b[0m )\n\u001b[1;32m---> 65\u001b[0m \u001b[39mreturn\u001b[39;00m \u001b[39msuper\u001b[39;49m()\u001b[39m.\u001b[39;49m\u001b[39m__call__\u001b[39;49m(iterable_with_config)\n",
      "File \u001b[1;32mc:\\Python310\\lib\\site-packages\\joblib\\parallel.py:1098\u001b[0m, in \u001b[0;36mParallel.__call__\u001b[1;34m(self, iterable)\u001b[0m\n\u001b[0;32m   1095\u001b[0m     \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_iterating \u001b[39m=\u001b[39m \u001b[39mFalse\u001b[39;00m\n\u001b[0;32m   1097\u001b[0m \u001b[39mwith\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_backend\u001b[39m.\u001b[39mretrieval_context():\n\u001b[1;32m-> 1098\u001b[0m     \u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49mretrieve()\n\u001b[0;32m   1099\u001b[0m \u001b[39m# Make sure that we get a last message telling us we are done\u001b[39;00m\n\u001b[0;32m   1100\u001b[0m elapsed_time \u001b[39m=\u001b[39m time\u001b[39m.\u001b[39mtime() \u001b[39m-\u001b[39m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_start_time\n",
      "File \u001b[1;32mc:\\Python310\\lib\\site-packages\\joblib\\parallel.py:975\u001b[0m, in \u001b[0;36mParallel.retrieve\u001b[1;34m(self)\u001b[0m\n\u001b[0;32m    973\u001b[0m \u001b[39mtry\u001b[39;00m:\n\u001b[0;32m    974\u001b[0m     \u001b[39mif\u001b[39;00m \u001b[39mgetattr\u001b[39m(\u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_backend, \u001b[39m'\u001b[39m\u001b[39msupports_timeout\u001b[39m\u001b[39m'\u001b[39m, \u001b[39mFalse\u001b[39;00m):\n\u001b[1;32m--> 975\u001b[0m         \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_output\u001b[39m.\u001b[39mextend(job\u001b[39m.\u001b[39;49mget(timeout\u001b[39m=\u001b[39;49m\u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49mtimeout))\n\u001b[0;32m    976\u001b[0m     \u001b[39melse\u001b[39;00m:\n\u001b[0;32m    977\u001b[0m         \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_output\u001b[39m.\u001b[39mextend(job\u001b[39m.\u001b[39mget())\n",
      "File \u001b[1;32mc:\\Python310\\lib\\site-packages\\joblib\\_parallel_backends.py:567\u001b[0m, in \u001b[0;36mLokyBackend.wrap_future_result\u001b[1;34m(future, timeout)\u001b[0m\n\u001b[0;32m    564\u001b[0m \u001b[39m\u001b[39m\u001b[39m\"\"\"Wrapper for Future.result to implement the same behaviour as\u001b[39;00m\n\u001b[0;32m    565\u001b[0m \u001b[39mAsyncResults.get from multiprocessing.\"\"\"\u001b[39;00m\n\u001b[0;32m    566\u001b[0m \u001b[39mtry\u001b[39;00m:\n\u001b[1;32m--> 567\u001b[0m     \u001b[39mreturn\u001b[39;00m future\u001b[39m.\u001b[39;49mresult(timeout\u001b[39m=\u001b[39;49mtimeout)\n\u001b[0;32m    568\u001b[0m \u001b[39mexcept\u001b[39;00m CfTimeoutError \u001b[39mas\u001b[39;00m e:\n\u001b[0;32m    569\u001b[0m     \u001b[39mraise\u001b[39;00m \u001b[39mTimeoutError\u001b[39;00m \u001b[39mfrom\u001b[39;00m \u001b[39me\u001b[39;00m\n",
      "File \u001b[1;32mc:\\Python310\\lib\\concurrent\\futures\\_base.py:446\u001b[0m, in \u001b[0;36mFuture.result\u001b[1;34m(self, timeout)\u001b[0m\n\u001b[0;32m    444\u001b[0m     \u001b[39mraise\u001b[39;00m CancelledError()\n\u001b[0;32m    445\u001b[0m \u001b[39melif\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_state \u001b[39m==\u001b[39m FINISHED:\n\u001b[1;32m--> 446\u001b[0m     \u001b[39mreturn\u001b[39;00m \u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49m__get_result()\n\u001b[0;32m    447\u001b[0m \u001b[39melse\u001b[39;00m:\n\u001b[0;32m    448\u001b[0m     \u001b[39mraise\u001b[39;00m \u001b[39mTimeoutError\u001b[39;00m()\n",
      "File \u001b[1;32mc:\\Python310\\lib\\concurrent\\futures\\_base.py:391\u001b[0m, in \u001b[0;36mFuture.__get_result\u001b[1;34m(self)\u001b[0m\n\u001b[0;32m    389\u001b[0m \u001b[39mif\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_exception:\n\u001b[0;32m    390\u001b[0m     \u001b[39mtry\u001b[39;00m:\n\u001b[1;32m--> 391\u001b[0m         \u001b[39mraise\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_exception\n\u001b[0;32m    392\u001b[0m     \u001b[39mfinally\u001b[39;00m:\n\u001b[0;32m    393\u001b[0m         \u001b[39m# Break a reference cycle with the exception in self._exception\u001b[39;00m\n\u001b[0;32m    394\u001b[0m         \u001b[39mself\u001b[39m \u001b[39m=\u001b[39m \u001b[39mNone\u001b[39;00m\n",
      "\u001b[1;31mMemoryError\u001b[0m: Unable to allocate 5.25 GiB for an array with shape (396, 1778337) and data type float64"
     ]
    }
   ],
   "source": [
    "from sklearn.model_selection import GridSearchCV\n",
    "\n",
    "best_params = []\n",
    "best_scores = []\n",
    "\n",
    "for classifier, param_grid in zip(classifiers, param_grids):\n",
    "    print(\"Classifier:\", classifier.__class__.__name__)\n",
    "    print(\"Parameters:\")\n",
    "    for key in param_grid:\n",
    "        print(f\"\\t{key:12}: {param_grid[key]}\")\n",
    "    print(\"\")\n",
    "    \n",
    "    grid_search = GridSearchCV(classifier, param_grid, cv=2, verbose=1, n_jobs=-1)\n",
    "    grid_search.fit(data.x_image_train, data.y_train)\n",
    "    best_params.append(grid_search.best_params_)\n",
    "    best_scores.append(grid_search.best_score_)\n",
    "    print(\"Best parameters:\", best_params[-1])\n",
    "    print(f\"Best score: {best_scores[-1]:.3f}\")\n",
    "    print(\"\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Printing the results"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Classifier: GradientBoostingClassifier\n",
      "Best parameters:\n",
      "\tcriterion   : friedman_mse\n",
      "\tlearning_rate: 0.05\n",
      "\tmax_depth   : 3\n",
      "\tmax_features: log2\n",
      "\tmin_samples_split: 10\n",
      "\tn_estimators: 100\n",
      "Best score: 0.782\n",
      "\n",
      "Classifier: AdaBoostClassifier\n",
      "Best parameters:\n",
      "\talgorithm   : SAMME.R\n",
      "\tlearning_rate: 0.01\n",
      "\tn_estimators: 500\n",
      "Best score: 0.535\n",
      "\n"
     ]
    }
   ],
   "source": [
    "for classifier, best_param, best_scores in zip(classifiers, best_params, best_scores):\n",
    "    print(\"Classifier:\", classifier.__class__.__name__)\n",
    "    print(\"Best parameters:\")\n",
    "    for key in best_param:\n",
    "        print(f\"\\t{key:12}: {best_param[key]}\")\n",
    "    print(f\"Best score: {best_scores:.3f}\")\n",
    "    print(\"\")"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
